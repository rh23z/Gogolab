#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Integrate HMMER *domtblout* files (Step3 detailed HMM â†’ integrate) â€” CLI version

åŠŸèƒ½ï¼š
- æ‰«æç»™å®šç›®å½•ï¼ˆå¯é€’å½’ï¼‰ä¸‹çš„æ‰€æœ‰ *.domtblout[.gz]* æ–‡ä»¶ï¼›
- å¹¶è¡Œè§£æï¼Œæ•´åˆä¸ºä¸€ä¸ª TSVï¼›
- åˆ—åé‡‡ç”¨ HMMER domtblout æ ‡å‡† 22 åˆ— + descriptionï¼›
- é¢å¤–é™„å¸¦åˆ—ï¼štarget_fileï¼ˆæ–‡ä»¶åæˆ–ç›¸å¯¹è·¯å¾„ï¼‰ã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š

python step3_s2_detailed_hmm_inte_cli.py \
  -i /work/zhangrh/couple_procject/result/cas9/flanking/Pfam-A \
  -o /work/zhangrh/couple_procject/result/cas9/flanking/step3_merged_pfam.tsv \
  --recursive --workers 128

å¯é€‰ï¼š
  --ext .domtblout        # é»˜è®¤
  --include-gz            # åŒæ—¶åŒ¹é… .domtblout.gz
  --relative-path         # target_file ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºè¾“å…¥æ ¹ç›®å½•ï¼‰

"""
from __future__ import annotations
import os
import gzip
import argparse
from pathlib import Path
import pandas as pd
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Iterable, List

# HMMER domtblout æ ‡å‡†å­—æ®µåˆ—è¡¨ï¼ˆ22 + descriptionï¼‰
COLNAMES = [
    "target_name", "target_accession", "tlen",
    "query_name", "query_accession", "qlen",
    "full_seq_Evalue", "full_seq_score", "full_seq_bias",
    "domain_num", "domain_of", "c_Evalue", "i_Evalue",
    "domain_score", "domain_bias",
    "hmm_from", "hmm_to", "ali_from", "ali_to", "env_from", "env_to",
    "acc", "description"
]


def _open_text(path: Path):
    """Open text file, supporting optional .gz."""
    if str(path).endswith('.gz'):
        return gzip.open(path, 'rt')
    return open(path, 'r')


def parse_domtblout(domtblout_path: Path) -> pd.DataFrame | None:
    try:
        with _open_text(domtblout_path) as f:
            lines = [line.strip() for line in f if not line.startswith('#') and line.strip()]
        if not lines:
            return None

        records: List[List[str]] = []
        for line in lines:
            parts = line.split()
            if len(parts) < 22:
                continue
            fixed = parts[:22]
            description = ' '.join(parts[22:])
            fixed.append(description)
            records.append(fixed)

        if not records:
            return None

        df = pd.DataFrame(records, columns=COLNAMES)
        return df
    except Exception as e:
        print(f"âŒ Error reading {domtblout_path}: {e}")
        return None


def _iter_domtblout_files(root: Path, recursive: bool, ext: str, include_gz: bool) -> Iterable[Path]:
    patterns = [f"*{ext}"]
    if include_gz:
        patterns.append(f"*{ext}.gz")
    if recursive:
        for pat in patterns:
            yield from root.rglob(pat)
    else:
        for pat in patterns:
            yield from root.glob(pat)


def main():
    ap = argparse.ArgumentParser(description='Integrate HMMER domtblout files into a single TSV (parallel).')
    ap.add_argument('-i', '--input-dir', required=True, help='è¾“å…¥ç›®å½•ï¼ŒåŒ…å« *.domtblout[.gz] æ–‡ä»¶')
    ap.add_argument('-o', '--output', required=True, help='è¾“å‡º TSV æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚ step3_merged_pfam.tsv)')
    ap.add_argument('--ext', default='.domtblout', help='æ–‡ä»¶æ‰©å±•åï¼ˆé»˜è®¤ .domtbloutï¼‰')
    ap.add_argument('--recursive', action='store_true', help='é€’å½’æŸ¥æ‰¾å­ç›®å½•')
    ap.add_argument('--include-gz', action='store_true', help='åŒæ—¶è§£æ .domtblout.gz')
    ap.add_argument('--workers', type=int, default=os.cpu_count() or 8, help='å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤=CPUæ ¸æ•°ï¼‰')
    ap.add_argument('--relative-path', action='store_true', help='target_file ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºè¾“å…¥ç›®å½•ï¼‰')
    args = ap.parse_args()

    in_root = Path(args.input_dir).resolve()
    out_path = Path(args.output).resolve()
    out_path.parent.mkdir(parents=True, exist_ok=True)

    domtblout_files = list(_iter_domtblout_files(in_root, args.recursive, args.ext, args.include_gz))
    if not domtblout_files:
        print(f"âš ï¸ No *{args.ext}[.gz] files found under: {in_root}")
        return

    print(f"ğŸ” Found {len(domtblout_files)} files. Parsing with workers={args.workers} â€¦")
    merged_df_list: List[pd.DataFrame] = []
    with ProcessPoolExecutor(max_workers=args.workers) as executor:
        futures = {executor.submit(parse_domtblout, p): p for p in domtblout_files}
        for fut in as_completed(futures):
            p = futures[fut]
            df = fut.result()
            if df is None:
                continue
            # é™„åŠ  target_file åˆ—
            df = df.copy()
            if args.relative_path:
                df['target_file'] = str(p.relative_to(in_root))
            else:
                df['target_file'] = p.name
            merged_df_list.append(df)

    if not merged_df_list:
        print("âš ï¸ No valid records to write.")
        return

    merged = pd.concat(merged_df_list, ignore_index=True)
    # è¾“å‡ºå•ä¸ª TSVï¼ˆé¿å…åŸè„šæœ¬ä¸­çš„åŒ .tsvï¼‰
    if not out_path.suffix:
        out_path = out_path.with_suffix('.tsv')
    merged.to_csv(out_path, sep='\t', index=False)
    print(f"âœ… Written to: {out_path}")


if __name__ == '__main__':
    main()
