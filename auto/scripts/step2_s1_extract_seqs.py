#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘½ä»¤è¡Œå·¥å…·ï¼šæ ¹æ® summary.tsv ä¸­çš„ source/file_name/seq_id ä¿¡æ¯ï¼Œ
ä»ä¸åŒç›®å½•çš„ fasta|faa æ–‡ä»¶é‡Œæå–ç›®æ ‡åºåˆ—ï¼Œæœ€ç»ˆåˆå¹¶è¾“å‡ºä¸€ä¸ª fastaã€‚
"""

import argparse
import sys
import os
from Bio import SeqIO
from concurrent.futures import ProcessPoolExecutor, as_completed
import pandas as pd

# source ä¸ fasta æ–‡ä»¶æ ¹ç›®å½•çš„æ˜ å°„
SOURCE_DICT = {
    'archaea_assembly_summary': "/work/data_share/NCBI_zrh/archaea_assembly_summary_prodigal/",
    'bacteria_assembly_summary': "/work/data_share/NCBI_zrh/bacteria_assembly_summary_prodigal/",
    'BGItrans': "/work/data/BGItrans/final_bins_faa/",
    'metagenomes_assembly_summary': "/work/data_share/NCBI_zrh/metagenomes_assembly_summary_prodigal/",
    'phagescope': '/work/data/phage/phagescope_all_prodigal/',
    'viral_assembly_summary': "/work/data_share/NCBI_zrh/viral_assembly_summary_prodigal/",
    'IMGM': '/work/data/IMGM_1025/faa/',
    'MGnify': '/work/data/MGnify/faa/',
    'CRBC': '/work/data/CRBC_genome/faa/',
    'IMGM_metagenome': '/work/data/IMGM_metagenome/faa/'
}


def extract_sequences(source, file_name, need_ids, log_file):
    root = SOURCE_DICT.get(source)
    if root is None:
        print(f"[WARN] æœªçŸ¥ source: {source}", file=sys.stderr)
        return []

    fasta_path = os.path.join(root, file_name)
    if not os.path.exists(fasta_path):
        print(f"[WARN] æ–‡ä»¶ä¸å­˜åœ¨: {fasta_path}", file=sys.stderr)
        return []

    out_records = []
    not_found = []

    for rec in SeqIO.parse(fasta_path, "fasta"):
        if rec.id in need_ids:
            out_records.append(rec)
            need_ids.remove(rec.id)

    not_found.extend(need_ids)
    if not_found:
        with open(log_file, 'a') as log:
            for seq_id in not_found:
                log.write(f"{source}\t{file_name}\t{seq_id}\n")

    return out_records


def main(args):
    input_tsv = args.input
    output_prefix = args.output_prefix
    log_dir = args.log_dir

    log_file = os.path.join(log_dir, "step2_s1_not_found.log")
    output_fasta = output_prefix+ ".fasta"

    print(f"ğŸ§¾ è¯»å–è¾“å…¥è¡¨ï¼š{input_tsv}")
    df = pd.read_table(input_tsv)
    df['target_file'] = df['target_file'].str.replace(".domtblout", ".faa", regex=False)
    need_dict = df.groupby(["source", "target_file"])["target_name"].apply(set).to_dict()

    print(f"ğŸš€ å¯åŠ¨å¹¶è¡Œæå–ï¼ˆçº¿ç¨‹æ•°: {args.threads}ï¼‰")
    all_records = []
    with ProcessPoolExecutor(max_workers=args.threads) as executor:
        futures = [executor.submit(extract_sequences, source, file_name, ids.copy(), log_file)
                   for (source, file_name), ids in need_dict.items()]
        for future in as_completed(futures):
            all_records.extend(future.result())

    print(f"ğŸ§¬ æå–å®Œæ¯•ï¼Œå…±æå–åºåˆ—ï¼š{len(all_records):,} æ¡")
    SeqIO.write(all_records, output_fasta, "fasta")
    print(f"âœ… è¾“å‡ºå†™å…¥ï¼š{os.path.abspath(output_fasta)}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä»å¤šä¸ªæ¥æºçš„ FASTA æ–‡ä»¶ä¸­æå–ç›®æ ‡åºåˆ—å¹¶è¾“å‡ºåˆå¹¶ FASTA")
    parser.add_argument('-i', '--input', required=True, help="åŒ…å« sourceã€fileã€seq_id çš„ tsv æ–‡ä»¶")
    parser.add_argument('-o', '--output_prefix', required=True, help="è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆç”¨äºä¿å­˜ .fasta å’Œæ—¥å¿—ï¼‰")
    parser.add_argument('-t', '--threads', type=int, default=32, help="å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 32ï¼‰")
    parser.add_argument('--log_dir', default='.', help="æ—¥å¿—æ–‡ä»¶ç›®å½•ï¼ˆé»˜è®¤: å½“å‰ç›®å½•ï¼‰")
    args = parser.parse_args()
    main(args)
